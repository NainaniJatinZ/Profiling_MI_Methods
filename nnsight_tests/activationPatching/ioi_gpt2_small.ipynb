{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "module_path = os.path.abspath(os.path.join('..', '..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "from nnsight import LanguageModel, util\n",
    "from nnsight.tracing.Proxy import Proxy\n",
    "from profiler import Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu, String: cpu\n",
      "Device: MPS, String: mps\n"
     ]
    }
   ],
   "source": [
    "profiler = Profiler()\n",
    "\n",
    "# Print available devices\n",
    "devices = profiler.available_devices()\n",
    "for device_name, device_str in devices.items():\n",
    "    print(f\"Device: {device_name}, String: {device_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zephyr/anaconda3/envs/profilemi/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load gpt2\n",
    "model = LanguageModel(\"openai-community/gpt2\", device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_prompt = [\"After John and Mary went to the store, Mary gave a bottle of milk to\"]\n",
    "corrupted_prompt = (\n",
    "    \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.input[1][\"input_ids\"].squeeze()\n",
    "print(clean_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' John': 1757\n",
      "' Mary': 5335\n"
     ]
    }
   ],
   "source": [
    "correct_index = model.tokenizer(\" John\")[\"input_ids\"][0]\n",
    "incorrect_index = model.tokenizer(\" Mary\")[\"input_ids\"][0]\n",
    "\n",
    "print(f\"' John': {correct_index}\")\n",
    "print(f\"' Mary': {incorrect_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__index__ returned non-int (type LanguageModelProxy)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m N_LAYERS \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mn_layer\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Enter nnsight tracing context\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m model\u001b[39m.\u001b[39mtrace() \u001b[39mas\u001b[39;00m tracer:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Clean run\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mwith\u001b[39;00m tracer\u001b[39m.\u001b[39minvoke(clean_prompt) \u001b[39mas\u001b[39;00m invoker:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         clean_tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minput[\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/anaconda3/envs/profilemi/lib/python3.12/site-packages/nnsight/contexts/Runner.py:41\u001b[0m, in \u001b[0;36mRunner.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"On exit, run and generate using the model whether locally or on the server.\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(exc_val, \u001b[39mBaseException\u001b[39;00m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mraise\u001b[39;00m exc_val\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremote:\n\u001b[1;32m     44\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_server()\n",
      "\u001b[1;32m/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m _ioi_patching_results \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Iterate through all tokens\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m token_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(clean_tokens)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39m# Patching corrupted run at given layer and token\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mwith\u001b[39;00m tracer\u001b[39m.\u001b[39minvoke(corrupted_prompt) \u001b[39mas\u001b[39;00m invoker:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m         \u001b[39m# Apply the patch from the clean hidden states to the corrupted hidden states.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         model\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mh[layer_idx]\u001b[39m.\u001b[39moutput[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mt[token_idx] \u001b[39m=\u001b[39m clean_hs[\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m             layer_idx\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zephyr/Documents/MechInt/Profiling_MI_Methods/nnsight_tests/activationPatching/ioi_gpt2_small.ipynb#W6sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m         ]\u001b[39m.\u001b[39mt[token_idx]\n",
      "\u001b[0;31mTypeError\u001b[0m: __index__ returned non-int (type LanguageModelProxy)"
     ]
    }
   ],
   "source": [
    "N_LAYERS = model.config.n_layer\n",
    "\n",
    "# Enter nnsight tracing context\n",
    "with model.trace() as tracer:\n",
    "\n",
    "    # Clean run\n",
    "    with tracer.invoke(clean_prompt) as invoker:\n",
    "        clean_tokens = model.input[1][\"input_ids\"].squeeze()\n",
    "\n",
    "        # Get hidden states of all layers in the network.\n",
    "        # We index the output at 0 because it's a tuple where the first index is the hidden state.\n",
    "        # No need to call .save() as we don't need the values after the run, just within the experiment run.\n",
    "        clean_hs = [\n",
    "            model.transformer.h[layer_idx].output[0]\n",
    "            for layer_idx in range(N_LAYERS)\n",
    "        ]\n",
    "\n",
    "        # Get logits from the lm_head.\n",
    "        clean_logits = model.lm_head.output\n",
    "\n",
    "        # Calculate the difference between the correct answer and incorrect answer for the clean run and save it.\n",
    "        clean_logit_diff = (\n",
    "            clean_logits[0, -1, correct_index] - clean_logits[0, -1, incorrect_index]\n",
    "        ).save()\n",
    "\n",
    "    # Corrupted run\n",
    "    with tracer.invoke(corrupted_prompt) as invoker:\n",
    "        corrupted_logits = model.lm_head.output\n",
    "\n",
    "        # Calculate the difference between the correct answer and incorrect answer for the corrupted run and save it.\n",
    "        corrupted_logit_diff = (\n",
    "            corrupted_logits[0, -1, correct_index]\n",
    "            - corrupted_logits[0, -1, incorrect_index]\n",
    "        ).save()\n",
    "\n",
    "    ioi_patching_results = []\n",
    "\n",
    "    # Iterate through all the layers\n",
    "    for layer_idx in range(len(model.transformer.h)):\n",
    "        _ioi_patching_results = []\n",
    "\n",
    "        # Iterate through all tokens\n",
    "        for token_idx in range(len(clean_tokens)):\n",
    "\n",
    "            # Patching corrupted run at given layer and token\n",
    "            with tracer.invoke(corrupted_prompt) as invoker:\n",
    "\n",
    "                # Apply the patch from the clean hidden states to the corrupted hidden states.\n",
    "                model.transformer.h[layer_idx].output[0].t[token_idx] = clean_hs[\n",
    "                    layer_idx\n",
    "                ].t[token_idx]\n",
    "\n",
    "                patched_logits = model.lm_head.output\n",
    "\n",
    "                patched_logit_diff = (\n",
    "                    patched_logits[0, -1, correct_index]\n",
    "                    - patched_logits[0, -1, incorrect_index]\n",
    "                )\n",
    "\n",
    "                # Calculate the improvement in the correct token after patching.\n",
    "                patched_result = (patched_logit_diff - corrupted_logit_diff) / (\n",
    "                    clean_logit_diff - corrupted_logit_diff\n",
    "                )\n",
    "\n",
    "                _ioi_patching_results.append(patched_result.save())\n",
    "\n",
    "        ioi_patching_results.append(_ioi_patching_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Clean logit difference: {clean_logit_diff.value:.3f}\")\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff.value:.3f}\")\n",
    "\n",
    "ioi_patching_results = util.apply(ioi_patching_results, lambda x: x.value.item(), Proxy)\n",
    "\n",
    "clean_tokens = [model.tokenizer.decode(token) for token in clean_tokens]\n",
    "token_labels = [f\"{token}_{index}\" for index, token in enumerate(clean_tokens)]\n",
    "\n",
    "fig = px.imshow(\n",
    "    ioi_patching_results,\n",
    "    color_continuous_midpoint=0.0,\n",
    "    color_continuous_scale=\"RdBu\",\n",
    "    labels={\"x\": \"Position\", \"y\": \"Layer\"},\n",
    "    x=token_labels,\n",
    "    title=\"Normalized Logit Difference After Patching Residual Stream on the IOI Task\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "profile_mi",
   "language": "python",
   "name": "profilemi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
